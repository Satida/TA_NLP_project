{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec 10 23:25:13 2019\n",
    "\n",
    "@author: satida\n",
    "\"\"\"\n",
    "\n",
    "from Utils.Utils import alphabet\n",
    "import persian\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def execute(self,docs):\n",
    "        \n",
    "        tokens_docs=list()\n",
    "        \n",
    "        for i,doc in enumerate(docs):\n",
    "            tokens=list()\n",
    "            doc=persian.convert_ar_characters(persian.convert_en_numbers(doc.replace(\"\\xa0\",\" \")))\n",
    "            if int(i/len(docs) * 100) % 10 == 0:\n",
    "                print(\"Tokenizer Complete in : \",int(i/len(docs) * 100) ,\"%\")\n",
    "            for word in doc.split() :\n",
    "                for ch in word :\n",
    "                    if ch not in alphabet or ch in persian.convert_en_numbers(\"0123456789\"):\n",
    "                        word=word.replace(ch,\"\")\n",
    "                if len(word)!=0:\n",
    "                    tokens.append(word)\n",
    "            tokens_docs.append(tokens)\n",
    "        return tokens_docs\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
